{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbec392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71a0eb",
   "metadata": {},
   "source": [
    "First we initiate all our imports and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ceebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('heart_2022_with_nans.csv')\n",
    "\n",
    "# Strip whitespace from string columns\n",
    "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Drop rows where target variable is missing\n",
    "df = df.dropna(subset=['HadHeartAttack'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b96f7",
   "metadata": {},
   "source": [
    "Reading in the csv file and stripping any unessicary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "df_model = df[['HadHeartAttack', 'Sex', 'AgeCategory', 'BMI', 'SmokerStatus', 'SleepHours']]\n",
    "\n",
    "# Drop rows with missing feature values\n",
    "df_model = df_model.dropna()\n",
    "\n",
    "# Separate X and y\n",
    "y = df_model['HadHeartAttack']\n",
    "X = df_model.drop('HadHeartAttack', axis=1)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937c3f6",
   "metadata": {},
   "source": [
    "selects specific features, drops missing values, separates inputs and target, and one-hot encodes categorical variables for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ea73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_encoded), columns=X_encoded.columns)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train **LogisticRegression** model\n",
    "model = LogisticRegression(max_iter=1000)  # optional: increase iterations if needed\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f7d08",
   "metadata": {},
   "source": [
    "scales the features, splits the data into training and testing sets, and trains a logistic regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate using classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=\"Yes\")\n",
    "recall = recall_score(y_test, y_pred, pos_label=\"Yes\")\n",
    "f1 = f1_score(y_test, y_pred, pos_label=\"Yes\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f25a1",
   "metadata": {},
   "source": [
    "Run the prediction model and print out its Accuracy, Precision, Recall, and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38838ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[\"No\", \"Yes\"])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb96dd",
   "metadata": {},
   "source": [
    "Run a table displaying the models confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26783286",
   "metadata": {},
   "source": [
    "What features seemed most important in your model? Why?\n",
    "    The features selected for the model (Sex, AgeCategory, BMI, SmokerStatus, SleepHours) are important as they are directly related to health and lifestyle factors that influence heart disease. BMI is related due to obesity being a known risk factor to heart disease. SleepHours because poor sleep is associated with health risks in general, AgeCategory as older folks are at higher risk of heart issues. Sex because biological differences may influence heart disease.\n",
    "\n",
    "Did one model perform better than the others? What trade-offs did you see?\n",
    "    I did not try multiple models so this is not applicable. \n",
    "\n",
    "What would you recommend to someone using this model to make real decisions?\n",
    "    Take it with a grain of salt. I am not a medical professional and would not be on the best standing to predict heart disease risks. The model was trained on a set of data that I cannot confirm the authenticity of. In a perfect world this data would be gathered by medical professionals across all regions of the world in order to gain a diverse set of data spanning all types of lifestyles and locations. \n",
    "\n",
    "What are the risks or limitations of this model in the real world?\n",
    "    High risk of genuine inaccuracy. While this model is considered accurate inside its own bubble of data it's been trained on. I would recommend having this model be trained on data gathered by professionals that we can verify the authenticity of. In the case that all of this data can have its authenticity verified I would say the limitations come down to the age group of people as well as the location as only a few states from the US were taken into consideration along with the virgin islands and Puerto Rico. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
